{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id=\"open-existing-db\"></a>\n",
    "###  1.2 Open an existing database\n",
    "To use an existing database, use the following call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import com.ibm.event.oltp.EventContext\n",
    "val eContext = EventContext.getEventContext(\"KillrWeather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val raw_weather_data = eContext.getTable(\"raw_weather_data\")\n",
    "val sky_condition_lookup = eContext.getTable(\"sky_condition_lookup\")\n",
    "val monthly_aggregate_precip = eContext.getTable(\"monthly_aggregate_precip\")\n",
    "val monthly_aggregate_windspeed = eContext.getTable(\"monthly_aggregate_windspeed\")\n",
    "val monthly_aggregate_pressure = eContext.getTable(\"monthly_aggregate_pressure\")\n",
    "val monthly_aggregate_temperature = eContext.getTable(\"monthly_aggregate_temperature\")\n",
    "val daily_aggregate_precip = eContext.getTable(\"daily_aggregate_precip\")\n",
    "val daily_aggregate_windspeed = eContext.getTable(\"daily_aggregate_windspeed\")\n",
    "val daily_aggregate_pressure = eContext.getTable(\"daily_aggregate_pressure\")\n",
    "val daily_aggregate_temperature = eContext.getTable(\"daily_aggregate_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"query-table\"></a>\n",
    "## 4. Query the table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-sqlContext\"></a>\n",
    "### 4.1 Create sqlContext using EventSession\n",
    "\n",
    "To run a Spark SQL query, you need to establish an IBM Db2 Event Store Spark session using sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.io.File\n",
    "import com.ibm.event.oltp.EventContext\n",
    "import org.apache.log4j.{Level, LogManager, Logger}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.ibm.event.EventSession\n",
    "\n",
    "val sqlContext = new EventSession(spark.sparkContext, \"KillrWeather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-DataFrame\"></a>\n",
    "### 4.2 Prepare a DataFrame for the query \n",
    "The following API provides a DataFrame that holds the query results on the IBM Db2 Event Store table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dfDailyTemp = sqlContext.loadEventTable(\"daily_aggregate_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- wsid: string (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- mean: double (nullable = false)\n",
      " |-- variance: double (nullable = false)\n",
      " |-- stdev: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDailyTemp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+---+----+----+------------------+------------------+------------------+\n",
      "|        wsid|year|month|day|high| low|              mean|          variance|             stdev|\n",
      "+------------+----+-----+---+----+----+------------------+------------------+------------------+\n",
      "|725030:14732|2008|    3|  1| 7.2| 0.0| 4.020833333333334|3.7516493055555546|1.9369174751536409|\n",
      "|725030:14732|2008|    3|  2| 6.1|-1.1|2.0708333333333333| 5.711232638888888|2.3898185368117155|\n",
      "|725030:14732|2008|    3|  3|11.1| 0.0| 6.179166666666666|11.096649305555557|3.3311633561798732|\n",
      "|725030:14732|2008|    3|  4|16.7| 0.0|             10.05|             11.49|3.3896902513356584|\n",
      "|725030:14732|2008|    3|  5|13.9| 0.0| 9.470833333333333|  8.09873263888889|2.8458272327899476|\n",
      "+------------+----+-----+---+----+----+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDailyTemp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lag\n",
    "import org.apache.spark.sql.functions.col \n",
    "\n",
    "val w = org.apache.spark.sql.expressions.Window.orderBy(\"year\", \"month\", \"day\")  \n",
    "\n",
    "val dfTrain = dfDailyTemp.withColumn(\"mean1\", lag(col(\"mean\"), 1, null).over(w)).\n",
    "    withColumn(\"mean2\", lag(col(\"mean\"), 2, null).over(w)).\n",
    "    withColumn(\"mean3\", lag(col(\"mean\"), 3, null).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+\n",
      "|               mean|             mean1|             mean2|             mean3|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "|  5.345833333333333|              null|              null|              null|\n",
      "| 1.5708333333333329| 5.345833333333333|              null|              null|\n",
      "|             -6.875|1.5708333333333329| 5.345833333333333|              null|\n",
      "| -3.287500000000001|            -6.875|1.5708333333333329| 5.345833333333333|\n",
      "|              2.925|-3.287500000000001|            -6.875|1.5708333333333329|\n",
      "|  4.887499999999999|             2.925|-3.287500000000001|            -6.875|\n",
      "|  9.508333333333333| 4.887499999999999|             2.925|-3.287500000000001|\n",
      "| 13.583333333333332| 9.508333333333333| 4.887499999999999|             2.925|\n",
      "| 12.112499999999999|13.583333333333332| 9.508333333333333| 4.887499999999999|\n",
      "|  8.254166666666666|12.112499999999999|13.583333333333332| 9.508333333333333|\n",
      "|  8.504166666666666| 8.254166666666666|12.112499999999999|13.583333333333332|\n",
      "|  8.283333333333331| 8.504166666666666| 8.254166666666666|12.112499999999999|\n",
      "|  5.195833333333334| 8.283333333333331| 8.504166666666666| 8.254166666666666|\n",
      "| 3.4250000000000007| 5.195833333333334| 8.283333333333331| 8.504166666666666|\n",
      "| 2.6791666666666667|3.4250000000000007| 5.195833333333334| 8.283333333333331|\n",
      "| 1.5624999999999998|2.6791666666666667|3.4250000000000007| 5.195833333333334|\n",
      "| 1.4208333333333334|1.5624999999999998|2.6791666666666667|3.4250000000000007|\n",
      "|              5.125|1.4208333333333334|1.5624999999999998|2.6791666666666667|\n",
      "| 2.5500000000000003|             5.125|1.4208333333333334|1.5624999999999998|\n",
      "|-1.4958333333333333|2.5500000000000003|             5.125|1.4208333333333334|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.select(\"mean\", \"mean1\", \"mean2\", \"mean3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.round\n",
    "\n",
    "val dfTrain2 = dfTrain.withColumn(\"mean\", round(col(\"mean\"), 1)).\n",
    "    withColumn(\"mean1\", round(col(\"mean1\"), 1)).\n",
    "    withColumn(\"mean2\", round(col(\"mean2\"), 1)).\n",
    "    withColumn(\"mean3\", round(col(\"mean3\"), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dfTrain3 = dfTrain2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+\n",
      "|mean|mean1|mean2|mean3|\n",
      "+----+-----+-----+-----+\n",
      "|-3.3| -6.9|  1.6|  5.3|\n",
      "| 2.9| -3.3| -6.9|  1.6|\n",
      "| 4.9|  2.9| -3.3| -6.9|\n",
      "| 9.5|  4.9|  2.9| -3.3|\n",
      "|13.6|  9.5|  4.9|  2.9|\n",
      "|12.1| 13.6|  9.5|  4.9|\n",
      "| 8.3| 12.1| 13.6|  9.5|\n",
      "| 8.5|  8.3| 12.1| 13.6|\n",
      "| 8.3|  8.5|  8.3| 12.1|\n",
      "| 5.2|  8.3|  8.5|  8.3|\n",
      "| 3.4|  5.2|  8.3|  8.5|\n",
      "| 2.7|  3.4|  5.2|  8.3|\n",
      "| 1.6|  2.7|  3.4|  5.2|\n",
      "| 1.4|  1.6|  2.7|  3.4|\n",
      "| 5.1|  1.4|  1.6|  2.7|\n",
      "| 2.6|  5.1|  1.4|  1.6|\n",
      "|-1.5|  2.6|  5.1|  1.4|\n",
      "|-5.9| -1.5|  2.6|  5.1|\n",
      "|-0.6| -5.9| -1.5|  2.6|\n",
      "| 3.1| -0.6| -5.9| -1.5|\n",
      "+----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain3.select(\"mean\", \"mean1\", \"mean2\", \"mean3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val splits = dfTrain3.randomSplit(Array(0.8, 0.20), seed = 24L)\n",
    "val training_data = splits(0)\n",
    "val test_data = splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler}\n",
    "\n",
    "val features_assembler = new VectorAssembler().\n",
    "    setInputCols(Array(\"mean1\", \"mean2\", \"mean3\" )).\n",
    "    setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().\n",
    "    setMaxIter(10).\n",
    "    setRegParam(0.3).\n",
    "    setElasticNetParam(0.8).\n",
    "    setLabelCol(\"mean\").\n",
    "    setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(features_assembler,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lrModel = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predictions = lrModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- wsid: string (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- variance: double (nullable = false)\n",
      " |-- stdev: double (nullable = false)\n",
      " |-- mean1: double (nullable = true)\n",
      " |-- mean2: double (nullable = true)\n",
      " |-- mean3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|mean|        prediction|\n",
      "+----+------------------+\n",
      "|-3.3|-2.180579955391931|\n",
      "| 9.5| 4.029306644077588|\n",
      "| 1.4|2.8647809037067287|\n",
      "| 5.1| 2.823364966767187|\n",
      "| 3.1|2.2094690231508345|\n",
      "| 1.2|  5.42483420588961|\n",
      "|11.7|  6.22901196815263|\n",
      "| 4.0| 5.046127730571579|\n",
      "|-0.8| 1.387744859726542|\n",
      "| 0.3|1.9901624523418961|\n",
      "| 2.1| 4.561894047529638|\n",
      "| 6.2|1.9074152459439286|\n",
      "| 4.0| 4.713169759256957|\n",
      "| 8.7| 8.960297693923128|\n",
      "| 7.6| 6.308917199374498|\n",
      "|12.4|10.428716454491164|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"mean\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "This demo introduced you to the IBM Db2 Event Store API for managing and querying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [IBM Db2 Event Store documentation](https://www.ibm.com/support/knowledgecenter/SSGNPV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2017. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
